{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scvi-tools.org/en/stable/user_guide/notebooks/api_overview.html  \n",
    "https://github.com/YosefLab/scvi-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install leidenalg\n",
    "# !pip install scanpy==1.7.0\n",
    "# !pip install scvi-tools\n",
    "#!pip install --user scikit-misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import argparse\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy as sp\n",
    "import scanpy as sc\n",
    "from collections import Counter\n",
    "import time\n",
    "import scvi\n",
    "import pickle\n",
    "import os\n",
    "import glob2\n",
    "plt.ion()\n",
    "plt.show()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_1c8', 'data_-1c4', 'data_-1c8', 'data_0c4', 'data_0c8', 'data_0c16', 'data_1.5c4', 'data_1c4', 'data_1.5c8', 'data_1.5c16', 'data_-1c16', 'data_1c16']\n",
      ">>>>dataset data_1c8\n",
      "\u001b[34mINFO    \u001b[0m No batch_key inputted, assuming all cells are same batch                            \n",
      "\u001b[34mINFO    \u001b[0m No label_key inputted, assuming all cells have same label                           \n",
      "\u001b[34mINFO    \u001b[0m Using data from adata.layers\u001b[1m[\u001b[0m\u001b[32m\"counts\"\u001b[0m\u001b[1m]\u001b[0m                                              \n",
      "\u001b[34mINFO    \u001b[0m Computing library size prior per batch                                              \n",
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;34m2000\u001b[0m cells, \u001b[1;34m2000\u001b[0m vars, \u001b[1;34m1\u001b[0m batches, \n",
      "         \u001b[1;34m1\u001b[0m labels, and \u001b[1;34m0\u001b[0m proteins. Also registered \u001b[1;34m0\u001b[0m extra categorical covariates and \u001b[1;34m0\u001b[0m extra\n",
      "         continuous covariates.                                                              \n",
      "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                          \n",
      "\u001b[34mINFO    \u001b[0m Training for \u001b[1;34m400\u001b[0m epochs                                                             \n",
      "\u001b[34mINFO    \u001b[0m KL warmup for \u001b[1;34m400\u001b[0m epochs                                                            \n",
      "Training...: 100%|██████████| 400/400 [00:53<00:00,  7.48it/s]\n",
      "\u001b[34mINFO    \u001b[0m Training time:  \u001b[1;34m53\u001b[0m s. \u001b[35m/\u001b[0m \u001b[1;34m400\u001b[0m epochs                                                  \n",
      "\u001b[34mINFO    \u001b[0m No batch_key inputted, assuming all cells are same batch                            \n",
      "\u001b[34mINFO    \u001b[0m No label_key inputted, assuming all cells have same label                           \n",
      "\u001b[34mINFO    \u001b[0m Using data from adata.layers\u001b[1m[\u001b[0m\u001b[32m\"counts\"\u001b[0m\u001b[1m]\u001b[0m                                              \n",
      "\u001b[34mINFO    \u001b[0m Computing library size prior per batch                                              \n",
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;34m2000\u001b[0m cells, \u001b[1;34m2000\u001b[0m vars, \u001b[1;34m1\u001b[0m batches, \n",
      "         \u001b[1;34m1\u001b[0m labels, and \u001b[1;34m0\u001b[0m proteins. Also registered \u001b[1;34m0\u001b[0m extra categorical covariates and \u001b[1;34m0\u001b[0m extra\n",
      "         continuous covariates.                                                              \n",
      "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                          \n",
      "\u001b[34mINFO    \u001b[0m Training for \u001b[1;34m400\u001b[0m epochs                                                             \n",
      "\u001b[34mINFO    \u001b[0m KL warmup for \u001b[1;34m400\u001b[0m epochs                                                            \n",
      "Training...:  91%|█████████▏| 365/400 [00:51<00:04,  7.11it/s]"
     ]
    }
   ],
   "source": [
    "for category in [  \"balanced_data\", \"imbalanced_data\", \"real_data\",\n",
    "                ]:\n",
    "\n",
    "    path= \"..\"\n",
    "    if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "        files = glob2.glob(f'{path}/R/simulated_data/{category}/*.h5')\n",
    "        files = [f[len(f\"{path}/R/simulated_data/{category}/\"):-3] for f in files]\n",
    "    else:\n",
    "        files = glob2.glob(f'{path}/real_data/*.h5')\n",
    "        files = [f[len(f\"{path}/real_data/\"):-3] for f in files]\n",
    "    print(files)\n",
    "\n",
    "    df = pd.DataFrame(columns = [\"dataset\", \"ARI\", \"NMI\", \"sil\", \"run\", \"time\", \"pred\", \"cal\"])\n",
    "    for dataset in files:\n",
    "        if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "            data_mat = h5py.File(f\"{path}/R/simulated_data/{category}/{dataset}.h5\",\"r\")\n",
    "        else:\n",
    "            data_mat = h5py.File(f\"{path}/real_data/{dataset}.h5\",\"r\")\n",
    "\n",
    "        Y = np.array(data_mat['Y'])\n",
    "        X = np.array(data_mat['X'])\n",
    "        print(f\">>>>dataset {dataset}\")\n",
    "\n",
    "        X = np.ceil(X).astype(np.int)\n",
    "        for run in range(3):\n",
    "            start = time.time()\n",
    "            adata = sc.AnnData(X)\n",
    "            adata.obs['Group'] = Y\n",
    "            adata.var_names_make_unique()\n",
    "\n",
    "\n",
    "            adata.layers[\"counts\"] = adata.X.copy() # preserve counts\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata.raw = adata # freeze the state in `.raw`\n",
    "            sc.pp.highly_variable_genes(\n",
    "                adata,\n",
    "                n_top_genes=2000,\n",
    "                subset=True,\n",
    "                flavor=\"seurat_v3\",\n",
    "                layer=\"counts\",\n",
    "                \n",
    "            )\n",
    "            scvi.data.setup_anndata(adata, layer=\"counts\")\n",
    "            model = scvi.model.SCVI(adata)\n",
    "            model.train()\n",
    "            latent = model.get_latent_representation()\n",
    "            adata.obsm[\"X_scVI\"] = latent\n",
    "            adata.layers[\"scvi_normalized\"] = model.get_normalized_expression(\n",
    "                library_size=10e4\n",
    "            )\n",
    "            \n",
    "            sc.pp.neighbors(adata, use_rep=\"X_scVI\")\n",
    "            sc.tl.umap(adata, min_dist=0.2)\n",
    "            sc.tl.leiden(adata, key_added=\"leiden_scVI\")\n",
    "\n",
    "\n",
    "            pred = adata.obs['leiden_scVI'].to_list()\n",
    "            pred = [int(x) for x in pred]\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            ARI = adjusted_rand_score(Y, pred)\n",
    "            NMI = np.around(normalized_mutual_info_score(Y, pred), 5)\n",
    "            ss = silhouette_score(adata.obsm[\"X_umap\"],pred)\n",
    "            cal = calinski_harabasz_score(adata.obsm[\"X_umap\"],pred)\n",
    "\n",
    "            df.loc[df.shape[0]] = [dataset, ARI, NMI, ss, run,elapsed, pred, cal]\n",
    "            df.to_pickle(f\"../output/pickle_results/{category}/{category}_scvi.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
